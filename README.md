# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #4 выполнил(а):
- Нестерова Анастасия Андреевна
- РИ210947
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Ознакомиться с основными операторами зыка Python на примере реализации линейной регрессии.

## Задание 1
### В проекте Unity реализовать перцептрон, который умеет производить вычисления.
Ход работы:
1) OR | дать комментарии о корректности работы

Подставляем такие значения, чтобы выполнялась логическая операция OR. Данные перцептрона совпадают с заведёнными нами.
![OR](https://user-images.githubusercontent.com/43472988/204355685-f59d4976-4d92-4fbf-9a7a-6e7bbe8ad2af.jpg)

2) AND | дать комментарии о корректности работы

Делаем всё тоже самое, подставляем такие значения, чтобы выполнялась логическая операция AND. Данные перцептрона совпадают с заведёнными нами.
![AND](https://user-images.githubusercontent.com/43472988/204355880-6ebd0b15-707a-425e-993d-3130dd8e3055.jpg)

3) NAND | дать комментарии о корректности работы

Подставляем такие значения, чтобы выполнялась логическая операция NAND. Данные перцептрона совпадают с заведёнными нами.
![NAND](https://user-images.githubusercontent.com/43472988/204356509-fd71e3fb-3fa3-4c57-8689-e43bd4a628dc.jpg)

4) XOR | дать комментарии о корректности работы

И наконец gодставляем такие значения, чтобы выполнялась логическая операция XOR. Как видно в приведённом ниже скрине, данные перцептрона не совпадают с данными, введёнными нами.
![XOR](https://user-images.githubusercontent.com/43472988/204356647-59b081a8-7795-4920-a5e4-1c15a9071c7a.jpg)

Попытаемся в скрипте изменить колличество иттераций с 8 на 15. Наблюдаем, что ситуация не изменилась, и данные, которые должны получиться после выполнения логической операции OR, получаются одними, а данные перцептрона - другими. Следовательно данная операция нелинейноотделимая, и перыептрон не может найти правильные данные.
![Перемена](https://user-images.githubusercontent.com/43472988/204360105-0b6cada3-d7da-4c2a-b176-34daa056f108.jpg)

## Задание 2

### В разделе "ход работы" пошагово выполнить каждый пункт с описанием и примером реализации задачи по теме лабораторной работы.
1)Произвести подготовку данных для работы с алгоритмом линейной регрессии. 10 видов данных были установлены случайным образом, и данные находились в линейной зависимости. Данные преобразуются в формат массива, чтобы их можно было вычислить напрямую при использовании умножения и сложения.
![Second 1](https://user-images.githubusercontent.com/43472988/191900528-d18e9e83-e2df-46ba-93ce-007dfb15396b.jpeg)
2)Определите связанные функции. Функция модели: определяет модель линейной регрессии wx+b. Функция потерь: функция потерь среднеквадратичной ошибки. Функция оптимизации: метод градиентного спуска для нахождения частных производных w и b.
![Second ex2](https://user-images.githubusercontent.com/43472988/192091453-23cc199e-0faf-451e-9b73-37d68705b130.jpg)
3)Начать итерацию
  
  Шаг 1 Инициализация и модель итеративной оптимизации
  ![Second ex3](https://user-images.githubusercontent.com/43472988/191902622-ff873825-3b72-4740-ad9c-2dc0e74b98da.jpg)
  Шаг 2 На второй итерации отображаются значения параметров, значения потерь и эффекты визуализации после итерации
  ![Second ex3](https://user-images.githubusercontent.com/43472988/192109714-15aeddb8-c120-4c64-bbd0-4ad4a43b8126.jpg)
  Шаг 3 Третья итерация показывает значения параметров, значения потерь и визуализацию после итерации
  ![Second ex4](https://user-images.githubusercontent.com/43472988/192109718-dafd79f3-40c1-4be8-819e-d9a28671bd50.jpg)
  Шаг 4 На четвертой итерации отображаются значения параметров, значения потерь и эффекты визуализации
  ![Second ex5](https://user-images.githubusercontent.com/43472988/192109736-a5130144-6e44-411a-addf-b4e8372b3df8.jpg)
  Шаг 5 Пятая итерация показывает значение параметра, значение потерь и эффект визуализации после итерации
  ![Second ex6](https://user-images.githubusercontent.com/43472988/192109756-6fc4ca46-2625-4bf4-9354-80564fcb66f7.jpg)
  Шаг 6 10000-я итерация, показывающая значения параметров, потери и визуализацию после итерации
  ![Second ex7](https://user-images.githubusercontent.com/43472988/192109766-f3ea3a9b-88b0-4276-b276-83c06f42b940.jpg)

## Задание 3 
### 1) Должна ли величина loss стремиться к нулю при изменении исходных данных? Ответьте на вопрос, привидите пример выполнения кода, который подтверждает ваш ответ
Производя итерации в предыдущем задании, можно увидеть, что каждый раз мы изменяем параметр times функции iterate. При этом величина loss не претерпевает особых изменений, из чего можно сделать вывод, что изменение исходных данных не влечёт за собой изменений величины loss, следовательно она стремится к нулю. Пример : 


    def iterate(a,b,x,y,times) :
      for i in range(times) :
      a,b=optimize(a,b,x,y)
      return a,b
    
    def loss_function(a,b,x,y):
      num = len(x)
      prediction = model(a,b,x)
      return (0.5 / num) * (np.square(prediction-y)).sum()

Видно, что количество итераций функции iterate(times) влияет на переменные a и b, а те в свою очередь определяют результат переменной loss.
Доказательство того, что от изменения количества итераций графики не изменялись, можно увидеть на скринах из второго задания к данным кускам кода:

    a, b = iterate(a, b, x, y, 2)
    prediction = model(a, b, x)
    loss = loss_function(a, b, x, y)
    print(a, b, loss)
    plt.scatter(x, y)
    plt.plot(x, prediction)

    a, b = iterate(a, b, x, y, 10000)
    prediction = model(a, b, x)
    loss = loss_function(a, b, x, y)
    print(a, b, loss)
    plt.scatter(x, y)
    plt.plot(x, prediction)

### 2) Какова роль параметра Lr? Ответьте на вопрос, приведите пример выполнения кода, который подтверждает ваш ответ. В качестве эксперимента можете изменить значение параметра.
    def optimize(a,b,x,y) :
      num = len(x)
      prediction = model(a,b,x)
      da=(1.0 / num)*((prediction -y)*x).sum()
      db=(1.0 / num)*((prediction - y).sum())
      a=a-Lr*da
      b=b-Lr*db
      return a,b

![Third ex1](https://user-images.githubusercontent.com/43472988/192372278-ad4cb408-9e9f-47a3-948c-fee7faaaa4be.jpg)
![Third ex2](https://user-images.githubusercontent.com/43472988/192372308-feb6c393-5b69-4205-9b7e-7341f97437ae.jpg)
![Third ex3](https://user-images.githubusercontent.com/43472988/192372323-bb4ce72b-db56-479e-b386-94917945b760.jpg)

Исходя из кода функции оптимизации и приведённых скринов, на которых можно наблюдать изменение параметра Lr, я могу сказать, что эта переменная отвечает за линейную регрессию, её направление. Как видно из графиков, при увеличении параметра Lr угол между прямой и осью координат увеличивается.

## Выводы
Работая над этой лабораторной работой, я познакомилась с такими средами разработки как Unity и Google Collab и написала в них элементарные программы, чтобы начать погружаться в изучение их инструментала, механик и возможностей. Кроме того я проанализировала заданный код по средствам внимательного изучения, небольших экспериментов, изменения переменных и наблюдения за поведением программы и полученных графиков, после чего сомгла сдеать некоторые выводы. 

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
